{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "faevDNp4sd64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "591b2362-3de3-41cf-ea2e-ac1bc4ddbac9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "#Install tqdm library for progress bars\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQDeeLLwi7D3"
      },
      "source": [
        "# The Activity Recognition Chain\n",
        "\n",
        "In this notebook, we will explore the [activity recognition chain ](https://dl.acm.org/doi/10.1145/2499621)to identify user activities using wrist-worn accelerometer data.\n",
        "\n",
        "## Before you begin\n",
        "1. Recall the steps of the activity recognition chain from the paper. We will be using a simplified version for this workshop. Recall the meaning of metrics used to evaluate classification performance.\n",
        "\n",
        "2. Read the whole notebook. This will give you an overview of where we are going.\n",
        "\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Activity recognition is the process of automatically identifying and classifying human activities based on sensor data. It has a wide range of applications, including health monitoring, fitness tracking, context-aware computing, and more. Wrist-worn accelerometers are commonly used sensors for activity recognition, as they can capture the motion patterns of the human body.\n",
        "\n",
        "In this notebook, we will work with the ADL Recognition with Wrist-worn Accelerometer Data Set. This dataset provides accelerometer recordings of various activities performed by 16 volunteers. The dataset includes a range of activities, such as walking, running, sitting, standing, climbing stairs, and more. Each activity has multiple recordings performed by the volunteers.\n",
        "\n",
        "## Learning outcomes\n",
        "By the end of this notebook, you will be able to implement an activity recognition chain in python.\n",
        "You will get to know and use the libraries typically used: pandas for reading and manipulating data, tsfresh for feature extraction and scikit-learn for model building.\n",
        "\n",
        "## Notebook Structure\n",
        "\n",
        "*Reading the dataset and Exploratory Data Analysis* We will start by exploring the structure and contents of the ADL Recognition dataset. We will visualize activity distributions, analyze accelerometer signals, and explore patterns in the data.\n",
        "\n",
        "*Feature Engineering:* Feature engineering is a crucial step in activity recognition. We will use tsfresh to obtain common statistical features. Read the documentation to get to know all the available features.\n",
        "\n",
        "*Model Training and Evaluation:* In this section, we will train and evaluate an activity recognition model using the preprocessed dataset and extracted features. We will explore two different machine learning algorithms, such as support vector machines (SVM) and decision trees to build our model. We will evaluate the model's performance leave-one-subject-out cross-validation and common metrics (accuracy, f1-score, precision, recall).\n",
        "\n",
        "During the notebook you will find **Activities**. Write your answers in a doc file and submit them in moodle.\n",
        "\n",
        "Let's get started with the dataset overview! First we will import some libraries that we will use to process the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MP2YbrBOgTi8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm #for progress bar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxA0buNaH7Vk"
      },
      "source": [
        "Download the dataset from https://archive.ics.uci.edu/static/public/283/dataset+for+adl+recognition+with+wrist+worn+accelerometer.zip. Name the file dataset.zip. Use the -q option to avoid output from wget command"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Gw8phFGuClch"
      },
      "outputs": [],
      "source": [
        "#Download dataset from https://archive.ics.uci.edu/static/public/283/dataset+for+adl+recognition+with+wrist+worn+accelerometer.zip and store in a file called dataset.zip\n",
        "# TODO hint: use !wget [URL]  -O [NAME] -q\n",
        "!wget https://archive.ics.uci.edu/static/public/283/dataset+for+adl+recognition+with+wrist+worn+accelerometer.zip -O adl_dataset -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSQeiOlCjoOA"
      },
      "source": [
        "## Reading the Dataset\n",
        "\n",
        "The ADL Recognition dataset is organized in a folder structure, where each activity has its own folder containing accelerometer recordings. The dataset follows a specific file naming convention: Accelerometer-[START_TIME]-[HMP]-[VOLUNTEER], where:\n",
        "\n",
        "[START_TIME] represents the timestamp of the starting moment of the recording in the format YYYY-MM-DD-HH-MM-SS.\n",
        "\n",
        "[HMP] is the name of the human motion pattern performed in the recorded trial.\n",
        "\n",
        "[VOLUNTEER] is the identification code of the volunteer performing the recorded motion, following the format [gN], where 'g' indicates the gender of the volunteer (m -> male, f -> female), and 'N' indicates the progressive number associated with the volunteer.\n",
        "\n",
        "The **read_file** function reads one file of the dataset while the **read_dataset** function reads the whole dataset, beginning with extracting all files from the zip folder.\n",
        "\n",
        "Notice how the information is extracted from the filename and how the pandas DataFrame is created by concatening all files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TgaW4xh_eIxu"
      },
      "outputs": [],
      "source": [
        "def read_file(file_path):\n",
        "    data = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            # Remove trailing newline character\n",
        "            line = line.rstrip('\\n')\n",
        "            # Split the line by space delimiter\n",
        "            values = line.split(' ')\n",
        "            # Convert the coded values to real acceleration values\n",
        "            real_values = [(-1.5 + (int(val) / 63) * 3) for val in values]\n",
        "            data.append(real_values)\n",
        "\n",
        "    return pd.DataFrame(data, columns=['X', 'Y', 'Z'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Rc0O8rqLfQvU"
      },
      "outputs": [],
      "source": [
        "def read_dataset(zip_file_path, extracted_folder_path, sampling_frequency):\n",
        "\n",
        "  # Create an empty DataFrame to store the dataset\n",
        "  df = pd.DataFrame()\n",
        "\n",
        "  # Extract the files from the zip file\n",
        "  with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "      zip_ref.extractall(extracted_folder_path)\n",
        "\n",
        "  # Iterate over all subdirectories inside the extracted folder\n",
        "  id = 0\n",
        "  for root, dirs, files in os.walk(extracted_folder_path):\n",
        "      for directory in tqdm(dirs, total=len(dirs)):  #Notice the use of tqdm to show progress bars\n",
        "          directory_path = os.path.join(root, directory)\n",
        "\n",
        "          # Check if the directory contains files starting with \"Accelerometer\"\n",
        "          if any(file.startswith('Accelerometer') for file in os.listdir(directory_path)):\n",
        "\n",
        "              # Iterate over the files in the directory\n",
        "              for filename in os.listdir(directory_path):\n",
        "                  if filename.startswith('Accelerometer'):\n",
        "                      file_path = os.path.join(directory_path, filename)\n",
        "\n",
        "                      # Extract information from the file name\n",
        "                      file_parts = filename.split('-')\n",
        "                      start_time = '-'.join(file_parts[1:7])\n",
        "                      volunteer = file_parts[-1].split('.')[0]\n",
        "                      label = file_parts[7]\n",
        "\n",
        "                      # Read the dataset from the file\n",
        "                      dataset = read_file(file_path)\n",
        "\n",
        "                      # Extract the start time from the filename\n",
        "                      start_time = pd.to_datetime(start_time, format='%Y-%m-%d-%H-%M-%S')\n",
        "\n",
        "\n",
        "                      # Calculate the time increment based on the sampling frequency\n",
        "                      time_increment = pd.to_timedelta(1 / sampling_frequency, unit='s')\n",
        "\n",
        "                      # Create a new column for the adjusted time\n",
        "                      dataset['Time'] = start_time + (pd.to_timedelta(dataset.index* time_increment, unit='s') )\n",
        "\n",
        "\n",
        "                      # Add the label, start time, and volunteer columns\n",
        "                      dataset['Start_Time'] = start_time\n",
        "                      dataset['Label'] = label\n",
        "                      dataset['Volunteer'] = volunteer\n",
        "                      dataset['id'] = id\n",
        "                      id += 1\n",
        "\n",
        "                      # Append the dataset to the DataFrame\n",
        "                      df = pd.concat([df, dataset])\n",
        "  return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZccSN9JIHZ6K"
      },
      "source": [
        "We have now created our two functions. Let's define values for the parameters, where the zip folder is located and where we want to extract the folder.  The sampling frequency will allow us to calculate the time of each datapoint.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Pn-KASbnmCrg"
      },
      "outputs": [],
      "source": [
        "# Zip file path - complete\n",
        "zip_file_path = '/content/adl_dataset'\n",
        "\n",
        "# Replace 'extracted_folder_path' with the desired path to extract the files\n",
        "extracted_folder_path = '/content/adl_dataset_extracted'\n",
        "\n",
        "sampling_frequency = 30"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOmkAlRAHM5i"
      },
      "source": [
        "We are now ready to read the dataset by calling our read_dataset function. Take a look at the final format of the data frame. What is the meaning of each column?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2K_1OJwsghRq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "outputId": "63712931-3225-48fe-ce64-d857c4b870a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 4888.47it/s]\n",
            "100%|██████████| 21/21 [00:12<00:00,  1.63it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          X         Y        Z                          Time  \\\n",
              "0  0.023810  0.166667  1.02381 2012-05-25 18:33:08.000000000   \n",
              "1  0.023810  0.309524  1.02381 2012-05-25 18:33:08.033333333   \n",
              "2  0.023810  0.309524  1.02381 2012-05-25 18:33:08.066666666   \n",
              "3  0.071429  0.261905  1.02381 2012-05-25 18:33:08.099999999   \n",
              "4  0.023810  0.166667  1.02381 2012-05-25 18:33:08.133333332   \n",
              "\n",
              "           Start_Time          Label Volunteer  id  \n",
              "0 2012-05-25 18:33:08  standup_chair        f4   0  \n",
              "1 2012-05-25 18:33:08  standup_chair        f4   0  \n",
              "2 2012-05-25 18:33:08  standup_chair        f4   0  \n",
              "3 2012-05-25 18:33:08  standup_chair        f4   0  \n",
              "4 2012-05-25 18:33:08  standup_chair        f4   0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-41df00ab-a3f5-4a56-a7a5-dca46a047412\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>X</th>\n",
              "      <th>Y</th>\n",
              "      <th>Z</th>\n",
              "      <th>Time</th>\n",
              "      <th>Start_Time</th>\n",
              "      <th>Label</th>\n",
              "      <th>Volunteer</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.023810</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>1.02381</td>\n",
              "      <td>2012-05-25 18:33:08.000000000</td>\n",
              "      <td>2012-05-25 18:33:08</td>\n",
              "      <td>standup_chair</td>\n",
              "      <td>f4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.023810</td>\n",
              "      <td>0.309524</td>\n",
              "      <td>1.02381</td>\n",
              "      <td>2012-05-25 18:33:08.033333333</td>\n",
              "      <td>2012-05-25 18:33:08</td>\n",
              "      <td>standup_chair</td>\n",
              "      <td>f4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.023810</td>\n",
              "      <td>0.309524</td>\n",
              "      <td>1.02381</td>\n",
              "      <td>2012-05-25 18:33:08.066666666</td>\n",
              "      <td>2012-05-25 18:33:08</td>\n",
              "      <td>standup_chair</td>\n",
              "      <td>f4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.071429</td>\n",
              "      <td>0.261905</td>\n",
              "      <td>1.02381</td>\n",
              "      <td>2012-05-25 18:33:08.099999999</td>\n",
              "      <td>2012-05-25 18:33:08</td>\n",
              "      <td>standup_chair</td>\n",
              "      <td>f4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.023810</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>1.02381</td>\n",
              "      <td>2012-05-25 18:33:08.133333332</td>\n",
              "      <td>2012-05-25 18:33:08</td>\n",
              "      <td>standup_chair</td>\n",
              "      <td>f4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-41df00ab-a3f5-4a56-a7a5-dca46a047412')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-41df00ab-a3f5-4a56-a7a5-dca46a047412 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-41df00ab-a3f5-4a56-a7a5-dca46a047412');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "#TODO create the df variable as the result of calling the read_dataset function. Pass the correct parameters to the function\n",
        "df = read_dataset(zip_file_path, extracted_folder_path, sampling_frequency)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSZAQTTTjf8R"
      },
      "source": [
        "## Exploratory Data Analysis (EDA)\n",
        "\n",
        "In this section, we will perform exploratory data analysis (EDA) on the accelerometer dataset. Our goal is to gain insights and understand the characteristics of the data.\n",
        "\n",
        "### Dataset Overview\n",
        "We have now stored the accelerometer recordings in a pandas DataFrame. Each row represents a specific timestamped measurement and contains the following columns:\n",
        "\n",
        "Index: The index of the measurement, it provides an order to the sequence.\n",
        "\n",
        "X, Y, Z: The accelerometer values along the X, Y, and Z axes, respectively.\n",
        "\n",
        "Label: The activity label associated with the measurement.\n",
        "\n",
        "id: it will help us identify each file (associated with one activity performed by one volunteer)\n",
        "\n",
        "volunteer: the id of the volunteer performing the activity\n",
        "\n",
        "Time: a timestamp for the recording\n",
        "\n",
        "Let's take a look at the dataset and see some plots of each activity. We will use the describe function to get a statistical summary of the data and then see how many recordings are there for each activity.\n",
        "\n",
        "We will use a [countplot](https://seaborn.pydata.org/generated/seaborn.countplot.html) (histogram) for this.\n",
        "\n",
        "See how different dataframe functions. You should remember these functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qiu8Uhi8ciy4"
      },
      "outputs": [],
      "source": [
        "#Libraries for plotting\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(df.head())\n",
        "\n",
        "# Check the shape of the dataset\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "\n",
        "# Summary statistics of the accelerometer data\n",
        "print(df.describe())\n",
        "\n",
        "# Plotting the distribution of activities\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(data=df, x='Label')\n",
        "plt.xlabel('Activity')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of Activities')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgB9UvX_InT-"
      },
      "source": [
        "Let's now plot some accelerometer recordings for each activity. Try to find some differences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpN-XOrteeoc"
      },
      "outputs": [],
      "source": [
        "# Get a unique list of activity labels\n",
        "activity_labels = df['Label'].unique()\n",
        "num_samples = 100\n",
        "\n",
        "# Plot one example of each activity\n",
        "plt.figure(figsize=(12, 8))\n",
        "for i, activity in enumerate(activity_labels):\n",
        "    sample_recording = df[df['Label'] == activity].head(num_samples)  # Select the first 100 recordings for each activity\n",
        "    plt.subplot(3, 5, i+1)\n",
        "    sns.lineplot(data=sample_recording, x=sample_recording.index, y='X', label='X')\n",
        "    sns.lineplot(data=sample_recording, x=sample_recording.index, y='Y', label='Y')\n",
        "    sns.lineplot(data=sample_recording, x=sample_recording.index, y='Z', label='Z')\n",
        "    plt.xlabel('Index')\n",
        "    plt.ylabel('Acceleration')\n",
        "    plt.title('Activity: {}'.format(activity))\n",
        "    plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dz8xYxg1I1Vq"
      },
      "source": [
        "We can also look at the magnitude instead of each axis separately. Let's see if you can find patterns more easily or not. Remember that the magnitude is orientation invariant meaning that it won't be affected by slight rotations of the sensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gs7n-g0VfXBI"
      },
      "outputs": [],
      "source": [
        "# Plot one example of each activity (magnitude)\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "df['Magnitude'] = #TODO calculate the magnitude as the square root of the sum of squares. For square root use numpy's sqrt function. To get the square of x use x**2.\n",
        "for i, activity in enumerate(activity_labels):\n",
        "    sample_recording = df[df['Label'] == activity].head(num_samples)  # Select the first 100 recordings for each activity\n",
        "    plt.subplot(3, 5, i+1)\n",
        "\n",
        "    sns.lineplot(data=sample_recording, x=sample_recording.index, y='Magnitude', label='Magnitude')\n",
        "    plt.xlabel('Index')\n",
        "    plt.ylabel('Acceleration')\n",
        "    plt.title('Activity: {}'.format(activity))\n",
        "    plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruXzvmEYqejo"
      },
      "source": [
        "# Exploring the walking activity\n",
        "Let's focus on walking for now. First, let's plot some examples of walking data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqyEKXUzeazv"
      },
      "outputs": [],
      "source": [
        "# Plot one example of each activity (magnitude)\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "example_data = df[df.Label=='walk']. #filter data to get only walking data\n",
        "for i in range(0, 15):\n",
        "    sample_recording = example_data.iloc[i*100:i*100+100,:]  # Select the first 100 recordings for each activity\n",
        "    plt.subplot(3, 5, i+1)\n",
        "\n",
        "    #TODO: WRITE THE CODE TO CREATE A LINEPLOT for the sample recording. X axis is the index, y axis is the magnitude. Add labels to the axis.\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lytRmqB4qaUo"
      },
      "source": [
        "Let's now see some statistics from  the walking records. Add comments to the code to describe what is happening in each line.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILymSR69JbfL"
      },
      "outputs": [],
      "source": [
        "all_walking_recordings = df[df['Label'] == 'walk']\n",
        "\n",
        "\n",
        "all_walking_recordings_groups = all_walking_recordings.groupby('id')\n",
        "\n",
        "all_walking_recordings_groups.size().head(30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzbV78L6t3pt"
      },
      "source": [
        "\n",
        "**Question:** What are the following statistics about?\n",
        "\n",
        "YOUR ANSWER GOES HERE IN TEXT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2DxGcmtt0a_"
      },
      "outputs": [],
      "source": [
        "all_walking_recordings_groups.size().describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bLqe3aJu0nz"
      },
      "source": [
        "Let's find the average \"walk\" session\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdDWtJC7rXDi"
      },
      "outputs": [],
      "source": [
        "all_walking_recordings['timestep'] = all_walking_recordings.groupby('id').cumcount() + 1\n",
        "# Group by hour and calculate average and standard deviation\n",
        "walking_avg = all_walking_recordings.groupby('timestep')['Magnitude'].mean()\n",
        "walking_std = all_walking_recordings.groupby('timestep')['Magnitude'].std()\n",
        "\n",
        "# Create a DataFrame with the results\n",
        "result_df = pd.DataFrame({'timestep': walking_avg.index, 'avg_value': walking_avg, 'std_value': walking_std})\n",
        "\n",
        "# Calculate upper and lower bounds for standard deviation\n",
        "result_df['upper'] = result_df['avg_value'] + result_df['std_value']\n",
        "result_df['lower'] = result_df['avg_value'] - result_df['std_value']\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(20, 6))\n",
        "plt.plot(result_df['timestep'], result_df['avg_value'], label='Average Value', color='blue')\n",
        "plt.fill_between(result_df['timestep'], result_df['lower'], result_df['upper'], alpha=0.2, color='blue', label='Standard Deviation')\n",
        "\n",
        "# Customize the plot (optional)\n",
        "plt.xlabel('timestep')\n",
        "plt.ylabel('Value')\n",
        "plt.title('Typical \"Walking session\" - Average and Standard Deviation')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_46-b4ygIvIS"
      },
      "outputs": [],
      "source": [
        "# Plot the average of each activity\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "for i, activity in enumerate(activity_labels):\n",
        "    all_recordings = df[df['Label'] == activity]\n",
        "    all_recordings['timestep'] = all_recordings.groupby('id').cumcount() + 1\n",
        "    # Group by hour and calculate average and standard deviation\n",
        "    activity_avg = all_recordings.groupby('timestep')['Magnitude'].mean()\n",
        "    activity_std = all_recordings.groupby('timestep')['Magnitude'].std()\n",
        "\n",
        "    # Create a DataFrame with the results\n",
        "    result_df = pd.DataFrame({'timestep': activity_avg.index, 'avg_value': activity_avg, 'std_value': activity_std})\n",
        "\n",
        "    # Calculate upper and lower bounds for standard deviation\n",
        "    result_df['upper'] = result_df['avg_value'] + result_df['std_value']\n",
        "    result_df['lower'] = result_df['avg_value'] - result_df['std_value']\n",
        "\n",
        "    # Create the plot\n",
        "    plt.subplot(3, 5, i+1)\n",
        "    plt.plot(result_df['timestep'], result_df['avg_value'], label='Average Value', color='blue')\n",
        "    plt.fill_between(result_df['timestep'], result_df['lower'], result_df['upper'], alpha=0.2, color='blue', label='Standard Deviation')\n",
        "\n",
        "    # Customize the plot (optional)\n",
        "    plt.xlabel('timestep')\n",
        "    plt.ylabel('Value')\n",
        "    plt.title(f'Avg. \"{activity} session\"')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLUKqMJgjPzz"
      },
      "source": [
        "### Statistical Summaries\n",
        "Next, we compute statistical summaries for the accelerometer values of each activity. This provides insights into the central tendency, dispersion, and range of the data for different activities. The statistical summaries include the count, mean, standard deviation, minimum, 25th percentile, median, 75th percentile, and maximum values.\n",
        "\n",
        "The statistical summaries enable us to compare and contrast the accelerometer values across different activities. We can observe variations in the magnitude and patterns of acceleration for each activity.\n",
        "\n",
        "We group by activity label and use the describe function to get these summaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wNKH6iphtlV"
      },
      "outputs": [],
      "source": [
        "# Group the data by activity and compute statistical summaries of X,Y and Z\n",
        "activity_stats = df.groupby('Label')[['X', 'Y', 'Z']].describe()\n",
        "print(activity_stats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpRx40OVh-FO"
      },
      "outputs": [],
      "source": [
        "# Group the data by activity and compute statistical summaries of the Magnitude\n",
        "activity_stats = #TODO\n",
        "print(activity_stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VYOur5XjInq"
      },
      "source": [
        "### Distribution Plot\n",
        "To visualize the distribution of accelerometer values for each activity, we create a seaborn distribution plot. This plot displays the density estimation of the accelerometer values along the X, Y, and Z axes, with each activity represented by a different color. The plot provides an overview of the distribution patterns and helps identify any significant differences or similarities between activities.\n",
        "\n",
        "By examining the distribution plot, we can understand the range and shape of the accelerometer values for each activity. This information is valuable for recognizing patterns and detecting anomalies within the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9QSs3NaiW75"
      },
      "outputs": [],
      "source": [
        "# Create a distribution plot for each activity\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(2, 2, 1)\n",
        "sns.kdeplot(data=df, x='X', hue='Label', fill=True, common_norm=False)\n",
        "plt.subplot(2, 2, 2)\n",
        "sns.kdeplot(data=df, x='Y', hue='Label', fill=True, common_norm=False)\n",
        "plt.subplot(2, 2, 3)\n",
        "sns.kdeplot(data=df, x='Z', hue='Label', fill=True, common_norm=False)\n",
        "plt.subplot(2, 2, 4)\n",
        "sns.kdeplot(data=df, x='Magnitude', hue='Label', fill=True, common_norm=False)\n",
        "plt.xlabel('Acceleration')\n",
        "plt.ylabel('Density')\n",
        "plt.title('Distribution of Accelerometer Values by Activity')\n",
        "plt.legend(title='Activity')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMrQfJSXj9y9"
      },
      "source": [
        "## Activity 1\n",
        "After exploring the dataset, answer the following questions\n",
        "\n",
        "\n",
        "1. What are the activities included in the accelerometer dataset?\n",
        "2. Can you describe the characteristics of each activity based on the sample recordings?\n",
        "3. How do the accelerometer values differ across different activities? Are there any noticeable patterns or trends?\n",
        "4. What insights can you gather from the statistical summaries of the accelerometer values for each activity?\n",
        "5. Are there any significant differences in the distribution of accelerometer values between activities? If so, what are they?\n",
        "6. Can you identify any outliers or anomalies in the accelerometer data? How might these affect the analysis or interpretation of the results? You might need to create additional plots for this question.\n",
        "7. How could the findings from the EDA be useful in developing an activity recognition model or application?\n",
        "9. How might the EDA findings be relevant in real-world scenarios, such as developing activity monitoring systems or fitness applications?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdEFk4Mvhaxq"
      },
      "source": [
        "# Obtaining features\n",
        "We are now ready to extract our features.\n",
        "We will use the tsfresh library for this. Run the following command to install it\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9MMevgok21w"
      },
      "outputs": [],
      "source": [
        "#Install tsfresh library to extract features\n",
        "!pip install tsfresh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiCb1wEKJGz3"
      },
      "source": [
        "tsfresh will extract features for each entity. An entity can be a time window, a volunteer, a sensor or the unit that you define to make each sample for your classification model. We will use each file in the dataset as a unit. This is the column id\n",
        "\n",
        "We also need to have the true labels for each sample. We will use the y array for this.\n",
        "\n",
        "Finally, we extract the volunteer to be able to do leave-one-subject"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HenF6aMaL9HK"
      },
      "outputs": [],
      "source": [
        "from tsfresh import extract_features\n",
        "from tsfresh import extract_relevant_features\n",
        "from tsfresh.utilities.dataframe_functions import roll_time_series\n",
        "\n",
        "#Prepare data for tsfresh. It needs an id column for each window. We will use each file as one window so the if is the same as the dataframe id.\n",
        "\n",
        "#we only need time, label and accelerometer values\n",
        "data = df[['X','Y','Z','id', 'Label']]\n",
        "\n",
        "# Prepare labels by keeping only one label per file (i.e. remove the duplicates for each file)\n",
        "y = data[['Label','id']].copy()\n",
        "y = y.set_axis(y.id)\n",
        "labels = y.drop_duplicates()\n",
        "labels = labels['Label']\n",
        "\n",
        "print(labels.shape)\n",
        "\n",
        "data = data.drop('Label', axis=1)\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcx_bBuTS9Fm"
      },
      "outputs": [],
      "source": [
        "# Specify which features you want to extract, and parameters for each of them\n",
        "feature_parameters = {\n",
        "    'mean': None,\n",
        "    'large_standard_deviation': [{'r':0.3}],\n",
        "    'standard_deviation':None,\n",
        "    'minimum': None,\n",
        "    'maximum': None,\n",
        "    'kurtosis': None,\n",
        "    'skewness': None\n",
        "}\n",
        "\n",
        "# Apply feature extraction using the tsfresh method\n",
        "extracted_features = extract_features(\n",
        "    data,\n",
        "    column_id='id',\n",
        "    default_fc_parameters=feature_parameters,\n",
        "    kind_to_fc_parameters={'': feature_parameters},\n",
        "    disable_progressbar=False\n",
        ")\n",
        "\n",
        "# Display the extracted features\n",
        "print(extracted_features.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBO-bL6BdQmb"
      },
      "outputs": [],
      "source": [
        "extracted_features.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rs6pKo1pJ14R"
      },
      "source": [
        "We are using only some features (specified in the feature_parameters).\n",
        "\n",
        "You could use the following code to extract all features available (over 1000)\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "Extract features from the rolled data using tsfresh\n",
        "\n",
        "extracted_features = extract_relevant_features(data, y, column_id='id')\n",
        "```\n",
        "\n",
        "Let's save the extracted features\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75UAU3boedZg"
      },
      "outputs": [],
      "source": [
        "extracted_features.to_csv('adl_features.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAw-e5snLqUd"
      },
      "source": [
        "## Building the model\n",
        "We are finally ready to build our activity recognition model. Since this is a classification problem, we can use any classification algorithm.\n",
        "\n",
        "We will use scikit-learn, a comprehensive machine learning library that provides various algorithms, evaluation metrics, and data preprocessing techniques.\n",
        "\n",
        "In this example, we will use the following models:\n",
        "\n",
        "*Support Vector Machine (SVM):* SVM is a powerful supervised learning algorithm used for classification tasks. It aims to find an optimal hyperplane that separates different classes by maximizing the margin between them.\n",
        "\n",
        "*Decision Tree:* Decision trees are hierarchical models that make decisions by partitioning the feature space into smaller regions based on the available features. These models are intuitive, interpretable, and capable of handling both numerical and categorical data.\n",
        "\n",
        "To evaluate the models, we will use **Leave-one-group-out cross-validation** which is a technique used to evaluate the performance of a model when dealing with grouped or clustered data. It ensures that each group (in our case, each volunteer) is left out once as the validation set, while the remaining groups are used for training.\n",
        "\n",
        "This method helps assess the model's generalization ability by simulating real-world scenarios where unseen groups (volunteers) are encountered during inference. This also ensures that we do not have any **data leakage** during training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LND64yazOLUF"
      },
      "source": [
        "Let's first create a list of the volunteers corresponding to each id (file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SUpz9ptOOAw"
      },
      "outputs": [],
      "source": [
        "group = df[['id', 'Volunteer']]\n",
        "group = group.drop_duplicates()\n",
        "group = group.set_index('id')\n",
        "group = group['Volunteer']\n",
        "group.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVlU_vG5TAan"
      },
      "outputs": [],
      "source": [
        "X = extracted_features\n",
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EK6PJdmSTEx5"
      },
      "outputs": [],
      "source": [
        "labels.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3B47xEe-Fe-7"
      },
      "outputs": [],
      "source": [
        "labels.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BC_pCvjmFfzX"
      },
      "outputs": [],
      "source": [
        "#check consistency\n",
        "assert(X.shape[0] == labels.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ky6SAReodmCO"
      },
      "outputs": [],
      "source": [
        "tabular_data = X.copy()\n",
        "tabular_data['label'] = labels\n",
        "tabular_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGe_ifOzSkSC"
      },
      "source": [
        "We will now build the model following these steps\n",
        "\n",
        "*Data Preparation:* The feature matrix 'X' and target vector 'y' should be prepared before running the code. Ensure that the 'volunteer' column is available in the dataset.\n",
        "\n",
        "*Cross-Validation Loop: *The model utilizes the LeaveOneGroupOut cross-validator provided by scikit-learn. It splits the data into training and testing sets, ensuring that each volunteer is left out once as the validation set.\n",
        "\n",
        "*Feature Scaling: *Before fitting the models, the feature matrix is standardized using the StandardScaler from scikit-learn. This ensures that all features have similar scales and prevents any particular feature from dominating the learning process.\n",
        "\n",
        "*Model Fitting and Prediction:* The SVM and Decision Tree models are trained on the scaled training data using the **fit() **method. The trained models are then used to predict the labels for the scaled test data using the predict() method.\n",
        "\n",
        "*Classification Reports:* After the cross-validation loop, the overall classification reports are computed using the true labels and predictions from all the folds. These reports provide important metrics such as precision, recall, and F1-score for each class, as well as an overall accuracy score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LToO6IcUPSs4"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import LeaveOneGroupOut\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def train_and_classify(X, y, group):\n",
        "\n",
        "  # Initialize SVM and Decision Tree classifiers\n",
        "  svm_model = SVC()\n",
        "  dt_model = DecisionTreeClassifier()\n",
        "\n",
        "  # Create empty lists to accumulate predictions and true labels\n",
        "  svm_predictions = []\n",
        "  dt_predictions = []\n",
        "  true_labels = []\n",
        "\n",
        "  # Create the LeaveOneGroupOut cross-validator based on 'Volunteer' column\n",
        "  logo = LeaveOneGroupOut()\n",
        "\n",
        "  n_groups = logo.get_n_splits(X, y, groups=group)\n",
        "\n",
        "  # Perform leave-one-subject-out cross-validation\n",
        "  for train_index, test_index in tqdm(logo.split(X, y, groups=group), total=n_groups):\n",
        "      # Split the data into training and testing sets\n",
        "      X_train, X_test = X[train_index], X[test_index]\n",
        "      y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "      # Create a scaler to standardize the features\n",
        "      scaler = StandardScaler()\n",
        "\n",
        "      # Standardize the feature matrix for training set. Notice that if we scale before separating, we are having a data leakage.\n",
        "      X_train_scaled = scaler.fit_transform(X_train)\n",
        "      X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "      # Fit the SVM model on the training data\n",
        "      svm_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "      # Predict the labels for the test data using SVM model\n",
        "      svm_pred = svm_model.predict(X_test_scaled)\n",
        "\n",
        "      # Accumulate the predictions and true labels\n",
        "      svm_predictions.extend(svm_pred)\n",
        "      true_labels.extend(y_test)\n",
        "\n",
        "      # Fit the Decision Tree model on the training data\n",
        "      dt_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "      # Predict the labels for the test data using Decision Tree model\n",
        "      dt_pred = dt_model.predict(X_test_scaled)\n",
        "\n",
        "      # Accumulate the predictions\n",
        "      dt_predictions.extend(dt_pred)\n",
        "\n",
        "  # Print the overall classification report for SVM model\n",
        "  print(\"SVM Classification Report:\")\n",
        "  print(classification_report(true_labels, svm_predictions))\n",
        "\n",
        "  # Print the overall classification report for Decision Tree model\n",
        "  print(\"Decision Tree Classification Report:\")\n",
        "  print(classification_report(true_labels, dt_predictions))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4zbtRZVI2Os"
      },
      "outputs": [],
      "source": [
        "train_and_classify(X.values, labels, group)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Znt0DoJhCJtA"
      },
      "source": [
        "# Using sliding windows\n",
        "In activity recognition, the sliding window technique plays a pivotal role by breaking down continuous sensor data into smaller segments. This enables the model to capture temporal patterns, ensuring a more comprehensive understanding of dynamic activities over time.\n",
        "\n",
        "A sliding window is a technique used in signal processing and time-series analysis. It involves moving a fixed-size window through a dataset, capturing a subset of the data within the window at each step. In the following illustration, the sliding window of size 4 moves one step at a time through the time series data, capturing a subset of four data points at each position. This process allows the model to analyze sequential patterns and relationships in the data.\n",
        "\n",
        "Original Time Series Data:\n",
        "```\n",
        "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
        "```\n",
        "\n",
        "Sliding Window (size = 4):\n",
        "\n",
        "```\n",
        "|---|---|---|---|\n",
        "1   2   3   4\n",
        "\n",
        "  |---|---|---|---|\n",
        "  2   3   4   5\n",
        "\n",
        "    |---|---|---|---|\n",
        "    3   4   5   6\n",
        "\n",
        "      |---|---|---|---|\n",
        "      4   5   6   7\n",
        "\n",
        "        |---|---|---|---|\n",
        "        5   6   7   8\n",
        "\n",
        "          |---|---|---|---|\n",
        "          6   7   8   9\n",
        "\n",
        "            |---|---|---|---|\n",
        "            7   8   9   10\n",
        "\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7reDPYGZRwlb"
      },
      "source": [
        "The following functions will create the windows. The line that does the windows is\n",
        "```\n",
        "strided = as_strided(a,shape = newshape,strides = newstrides)\n",
        "```\n",
        "This is a function from numpy. Read the documentation and explain what this function does."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJd35_R7CI1z"
      },
      "outputs": [],
      "source": [
        "#code from https://github.com/ubicompsoartutorial/soar_tutorial/blob/main/ecdf/sliding_window.py\n",
        "import numpy as np\n",
        "from numpy.lib.stride_tricks import as_strided as ast\n",
        "\n",
        "def norm_shape(shape):\n",
        "    '''\n",
        "    Normalize numpy array shapes so they're always expressed as a tuple,\n",
        "    even for one-dimensional shapes.\n",
        "\n",
        "    Parameters\n",
        "        shape - an int, or a tuple of ints\n",
        "\n",
        "    Returns\n",
        "        a shape tuple\n",
        "    '''\n",
        "    try:\n",
        "        i = int(shape)\n",
        "        return (i,)\n",
        "    except TypeError:\n",
        "        # shape was not a number\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        t = tuple(shape)\n",
        "        return t\n",
        "    except TypeError:\n",
        "        # shape was not iterable\n",
        "        pass\n",
        "\n",
        "    raise TypeError('shape must be an int, or a tuple of ints')\n",
        "\n",
        "def sliding_window(a,ws,ss = None,flatten = True):\n",
        "    '''\n",
        "    Return a sliding window over a in any number of dimensions\n",
        "\n",
        "    Parameters:\n",
        "        a  - an n-dimensional numpy array\n",
        "        ws - an int (a is 1D) or tuple (a is 2D or greater) representing the size\n",
        "             of each dimension of the window\n",
        "        ss - an int (a is 1D) or tuple (a is 2D or greater) representing the\n",
        "             amount to slide the window in each dimension. If not specified, it\n",
        "             defaults to ws.\n",
        "        flatten - if True, all slices are flattened, otherwise, there is an\n",
        "                  extra dimension for each dimension of the input.\n",
        "\n",
        "    Returns\n",
        "        an array containing each n-dimensional window from a\n",
        "    '''\n",
        "\n",
        "    if None is ss:\n",
        "        # ss was not provided. the windows will not overlap in any direction.\n",
        "        ss = ws\n",
        "    ws = norm_shape(ws)\n",
        "    ss = norm_shape(ss)\n",
        "\n",
        "    # convert ws, ss, and a.shape to numpy arrays so that we can do math in every\n",
        "    # dimension at once.\n",
        "    ws = np.array(ws)\n",
        "    ss = np.array(ss)\n",
        "    shape = np.array(a.shape)\n",
        "\n",
        "    # ensure that ws, ss, and a.shape all have the same number of dimensions\n",
        "    ls = [len(shape),len(ws),len(ss)]\n",
        "    if 1 != len(set(ls)):\n",
        "        raise ValueError(\\\n",
        "        'a.shape, ws and ss must all have the same length. They were %s' % str(ls))\n",
        "\n",
        "    # ensure that ws is smaller than a in every dimension\n",
        "    if np.any(ws > shape):\n",
        "        raise ValueError(\\\n",
        "        'ws cannot be larger than a in any dimension.\\\n",
        " a.shape was %s and ws was %s' % (str(a.shape),str(ws)))\n",
        "\n",
        "    # how many slices will there be in each dimension?\n",
        "    newshape = norm_shape(((shape - ws) // ss) + 1)\n",
        "    # the shape of the strided array will be the number of slices in each dimension\n",
        "    # plus the shape of the window (tuple addition)\n",
        "    newshape += norm_shape(ws)\n",
        "    # the strides tuple will be the array's strides multiplied by step size, plus\n",
        "    # the array's strides (tuple addition)\n",
        "    newstrides = norm_shape(np.array(a.strides) * ss) + a.strides\n",
        "    strided = ast(a,shape = newshape,strides = newstrides)\n",
        "    if not flatten:\n",
        "        return strided\n",
        "\n",
        "    # Collapse strided so that it has one more dimension than the window.  I.e.,\n",
        "    # the new array is a flat list of slices.\n",
        "    meat = len(ws) if ws.shape else 0\n",
        "    firstdim = (np.product(newshape[:-meat]),) if ws.shape else ()\n",
        "    dim = firstdim + (newshape[-meat:])\n",
        "    # remove any dimensions with size 1\n",
        "    # dim = filter(lambda i : i != 1,dim)\n",
        "    return strided.reshape(dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQJI6MEUD5QC"
      },
      "outputs": [],
      "source": [
        "def perform_sliding_window(data_x,\n",
        "                           data_y,\n",
        "                           ws,\n",
        "                           ss):\n",
        "    \"\"\"\n",
        "    Efficiently performing the sliding window based segmentation\n",
        "    :param data_x: processed data stream\n",
        "    :param data_y: processed labels stream\n",
        "    :param ws: window size\n",
        "    :param ss: overlap size\n",
        "    :return: windowed data and ground truth labels\n",
        "    \"\"\"\n",
        "    data_x = sliding_window(data_x, (ws, data_x.shape[1]), (ss, 1))\n",
        "\n",
        "    # Just making it a vector if it was a 2D matrix\n",
        "    data_y = np.reshape(data_y, (len(data_y),))\n",
        "    data_y = np.asarray([[i[-1]] for i in sliding_window(data_y, ws, ss)])\n",
        "    return data_x.astype(np.float32), data_y.reshape(len(data_y)). \\\n",
        "        astype(np.uint8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIQ0uKUGSOeM"
      },
      "source": [
        "Run the following line to understand the code. Input parameters are the accelerometer data (a 2D matrix) and the activity labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfNocpEZSjl5"
      },
      "outputs": [],
      "source": [
        "# Create a 2D matrix of size 20x3 with random numbers between 0 and 100\n",
        "random_x = np.random.randint(0, 101, size=(20, 3))\n",
        "\n",
        "# Create a vector of size 20 with random integers between 0 and 3\n",
        "random_y = np.random.randint(0, 4, size=20)\n",
        "\n",
        "print(random_x)\n",
        "\n",
        "print(\"Windows\")\n",
        "\n",
        "win_x, win_y = perform_sliding_window(random_x, random_y, 10, 5)\n",
        "print(win_x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6tK8SfWTU6j"
      },
      "source": [
        "## Questions\n",
        "1. What is the size of the original data (random_x)?\n",
        "2. What is the size of each window?\n",
        "3. What is the shape of the result (win_x) ?\n",
        "4. What line is the beginning of the second window?\n",
        "5. What is the meaning of the overlap parameter (ss)?\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ct6qG_58Tw8x"
      },
      "source": [
        "Let's now create some windows for the dataset. For each window, we will obtain the features we calculated before (mean, median, mode ... ). Each window will now become one sample for the classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZIWNtn-EG-i"
      },
      "outputs": [],
      "source": [
        "WINDOW_SIZE = 30  # 1 second of data\n",
        "OVERLAP = 15  # 50% overlap, as is typical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZQJ49SJRfZW"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import mode\n",
        "\n",
        "def feature_representation(x):\n",
        "  #assert what is the expected shape of x?\n",
        "  #TODO\n",
        "\n",
        "  # Calculate statistical features\n",
        "  mean_values = np.mean(x, axis=0)\n",
        "  mode_values = mode(x, axis=0).mode\n",
        "\n",
        "  median_values = np.median(x, axis=0)\n",
        "\n",
        "  max_values = np.max(x, axis=0)\n",
        "  min_values = np.min(x, axis=0)\n",
        "  q75, q25 = np.percentile(x, [75, 25], axis=0)\n",
        "  iqr_values = q75 - q25\n",
        "\n",
        "  # Concatenate all the values into a single array\n",
        "  features = np.concatenate([mean_values, mode_values, median_values, max_values, min_values, iqr_values])\n",
        "\n",
        "  # Return the results\n",
        "  return features\n",
        "\n",
        "def get_features(x):\n",
        "  num_windows = x.shape[0]\n",
        "  num_components = x.shape[2]\n",
        "  temp_features = np.zeros((num_windows, (num_components) * 6))\n",
        "\n",
        "  # Compute features for each windows using feature_representation\n",
        "  for i in range(num_windows):\n",
        "    temp_features[i] = feature_representation(x[i])\n",
        "\n",
        "  # Return values\n",
        "  return temp_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "046Nn4xKKhtC"
      },
      "outputs": [],
      "source": [
        "data = df[['X','Y','Z', 'Label', 'Volunteer']]\n",
        "\n",
        "#encode labels - convert them to numbers\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y = df['Label'].values\n",
        "le.fit(y)\n",
        "\n",
        "labels = le.transform(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3XdwyFFKysi"
      },
      "outputs": [],
      "source": [
        "print(data.shape)\n",
        "print(labels.shape)\n",
        "print(group.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uBb2egKSabb"
      },
      "outputs": [],
      "source": [
        "#get sliding window and features per user\n",
        "users = df['Volunteer'].unique()\n",
        "user_le = LabelEncoder()\n",
        "user_le.fit(users)\n",
        "users_num = user_le.transform(users)\n",
        "\n",
        "all_data = []\n",
        "all_labels = []\n",
        "all_groups = []\n",
        "for i in tqdm(range(len(users))):\n",
        "  user_data = data[data['Volunteer']==users[i]]\n",
        "  x = user_data[['X', 'Y', 'Z']].values\n",
        "  y = user_data['Label'].values\n",
        "  y = le.transform(y)\n",
        "  win_x, win_y = perform_sliding_window(\n",
        "              x,\n",
        "              y,\n",
        "              WINDOW_SIZE,\n",
        "              OVERLAP\n",
        "          )\n",
        "  #get features\n",
        "  features = get_features(win_x)\n",
        "  all_data.append(features)\n",
        "  all_labels.append(win_y)\n",
        "\n",
        "  groups = np.full_like(win_y, fill_value=users_num[i])\n",
        "  all_groups.append(groups)\n",
        "\n",
        "#train and classify with the new windows\n",
        "X = np.concatenate(all_data)\n",
        "y = np.concatenate(all_labels)\n",
        "groups = np.concatenate(all_groups)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rt-B8Di5bXhh"
      },
      "outputs": [],
      "source": [
        "assert X.shape[0] == y.shape[0] == groups.shape[0]\n",
        "\n",
        "print(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbkl_DmwCW8W"
      },
      "outputs": [],
      "source": [
        "train_and_classify(X,y, groups)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vo9Avj8LdRd"
      },
      "source": [
        "# Homework\n",
        "\n",
        "1. We didn't filter the signal. Include a filter and see if the classification performance improves.\n",
        "\n",
        "2. Sliding windows can have different sizes.\n",
        "Make experiments to evaluate which window size has better performance. Select 3 window sizes and provide recommendations based on the type of activity, number of examples, etc.\n",
        "\n",
        "[OPTIONAL] : Try different classifiers. Random Forest and XGBoost tend to have improved performance.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}